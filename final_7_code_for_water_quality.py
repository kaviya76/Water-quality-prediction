# -*- coding: utf-8 -*-
"""FINAL 7 CODE FOR WATER QUALITY

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zWG7gfNWkMkRpnpa7DSrMsNx21yiHpoa
"""

import numpy as np
import pandas as pd
import os
for dirname, _, filenames in os.walk('/content/9000.csv'):

 print(os.path.join(dirname, filename))

# It is always consider as a good practice to make a copy of original dataset.

main_df = pd.read_csv('/content/9000.csv')
df = main_df.copy()

df.head()

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import seaborn as sns
from matplotlib import pyplot as plt
import matplotlib.pyplot as plt
# %matplotlib inline
import plotly.express as px
import warnings
warnings.filterwarnings('ignore')

print(df.shape)

print(df.columns)

df.describe()

df.info()

print(df.nunique())

print(df.isnull().sum())

df.dtypes

sns.heatmap(df.isnull())

plt.figure(figsize=(10, 8))
sns.heatmap(df.corr(), annot= True, cmap='coolwarm')

corr = df.corr()
c1 = corr.abs().unstack()
c1.sort_values(ascending = False)[12:24:2]

ax = sns.countplot(x = "Potability",data= df, saturation=0.8)
plt.xticks(ticks=[0, 1], labels = ["Not Potable", "Potable"])
plt.show()

x = df.Potability.value_counts()
labels = [0,1]
print(x)

# Visualizing dataset and also checking for outliers

fig, ax = plt.subplots(ncols = 5, nrows = 2, figsize = (20, 10))
index = 0
ax = ax.flatten()

for col, value in df.items():
    sns.boxplot(y=col, data=df, ax=ax[index])
    index += 1
plt.tight_layout(pad = 0.5, w_pad=0.7, h_pad=5.0)

plt.rcParams['figure.figsize'] = [20,10]
df.hist()
plt.show()

sns.pairplot(df, hue="Potability")

plt.rcParams['figure.figsize'] = [7,5]
sns.distplot(df['Potability'])

df.hist(column='ph', by='Potability')

df.hist(column='Hardness', by='Potability')

# Individual box plot for each feature
def Box(df):
    plt.title("Box Plot")
    sns.boxplot(df)
    plt.show()
Box(df['ph'])

sns.histplot(x = "Hardness", data=df)

df.nunique()

skew_val = df.skew().sort_values(ascending=False)
skew_val

fig = px.box(df, x="Potability", y="ph", color="Potability", width=800, height=400)
fig.show()

fig = px.box(df, x="Potability", y="Hardness", color="Potability", width=800, height=400)
fig.show()

fig = px.histogram (df, x = "Sulfate",  facet_row = "Potability",  template = 'plotly_dark')
fig.show ()

fig = px.histogram (df, x = "Trihalomethanes",  facet_row = "Potability",  template = 'plotly_dark')
fig.show ()

fig =  px.pie (df, names = "Potability", hole = 0.4, template = "plotly_dark")
fig.show ()

fig = px.scatter (df, x = "ph", y = "Sulfate", color = "Potability", template = "plotly_dark",  trendline="ols")
fig.show ()

fig = px.scatter (df, x = "Organic_carbon", y = "Hardness", color = "Potability", template = "plotly_dark",  trendline="lowess")
fig.show ()

df.isnull().mean().plot.bar(figsize=(10,6))
plt.ylabel('Percentage of missing values')
plt.xlabel('Features')
plt.title('Missing Data in Percentages');

df['ph'] = df['ph'].fillna(df['ph'].mean())
df['Sulfate'] = df['Sulfate'].fillna(df['Sulfate'].mean())
df['Trihalomethanes'] = df['Trihalomethanes'].fillna(df['Trihalomethanes'].mean())

df.head()

sns.heatmap(df.isnull())

df.isnull().sum()

X = df.drop('Potability', axis=1)
y = df['Potability']

X.shape, y.shape

# import StandardScaler to perform scaling
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

X = scaler.fit_transform(X)
X

# import train-test split
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report

# Creating model object
model_lg = LogisticRegression(max_iter=120,random_state=0, n_jobs=20)

# Training Model
model_lg.fit(X_train, y_train)

# Making Prediction
pred_lg = model_lg.predict(X_test)

# Calculating Accuracy Score
lg = accuracy_score(y_test, pred_lg)
print(lg)

print(classification_report(y_test,pred_lg))

# confusion Maxtrix
cm1 = confusion_matrix(y_test, pred_lg)
sns.heatmap(cm1/np.sum(cm1), annot = True, fmt=  '0.2%', cmap = 'Greens')

from sklearn.tree import DecisionTreeClassifier

# Creating model object
model_dt = DecisionTreeClassifier( max_depth=4, random_state=42)

# Training Model
model_dt.fit(X_train,y_train)

# Making Prediction
pred_dt = model_dt.predict(X_test)

# Calculating Accuracy Score
dt = accuracy_score(y_test, pred_dt)
print(dt)

print(classification_report(y_test,pred_dt))

# confusion Maxtrix
cm2 = confusion_matrix(y_test, pred_dt)
sns.heatmap(cm2/np.sum(cm2), annot = True, fmt=  '0.2%', cmap = 'Greens')

from sklearn.ensemble import RandomForestClassifier

# Creating model object
model_rf = RandomForestClassifier(n_estimators=300,min_samples_leaf=0.16, random_state=42)

# Training Model
model_rf.fit(X_train, y_train)

# Making Prediction
pred_rf = model_rf.predict(X_test)

# Calculating Accuracy Score
rf = accuracy_score(y_test, pred_rf)
print(rf)

print(classification_report(y_test,pred_rf))

# confusion Maxtrix
cm3 = confusion_matrix(y_test, pred_rf)
sns.heatmap(cm3/np.sum(cm3), annot = True, fmt=  '0.2%', cmap = 'Greens')

from xgboost import XGBClassifier

# Creating model object
model_xgb = XGBClassifier(max_depth= 8, n_estimators= 125, random_state= 0,  learning_rate= 0.03, n_jobs=5)

# Training Model
model_xgb.fit(X_train, y_train)

# Making Prediction
pred_xgb = model_xgb.predict(X_test)

# Calculating Accuracy Score
xgb = accuracy_score(y_test, pred_xgb)
print(xgb)

print(classification_report(y_test,pred_xgb))

# confusion Maxtrix
cm4 = confusion_matrix(y_test, pred_xgb)
sns.heatmap(cm4/np.sum(cm4), annot = True, fmt=  '0.2%', cmap = 'Greens')

from sklearn.neighbors import KNeighborsClassifier

# Creating model object
model_kn = KNeighborsClassifier(n_neighbors=9, leaf_size=20)

# Training Model
model_kn.fit(X_train, y_train)

# Making Prediction
pred_kn = model_kn.predict(X_test)

# Calculating Accuracy Score
kn = accuracy_score(y_test, pred_kn)
print(kn)

print(classification_report(y_test,pred_kn))

# confusion Maxtrix
cm5 = confusion_matrix(y_test, pred_kn)
sns.heatmap(cm5/np.sum(cm5), annot = True, fmt=  '0.2%', cmap = 'Greens')

from sklearn.svm import SVC, LinearSVC

model_svm = SVC(kernel='rbf', random_state = 42)

model_svm.fit(X_train, y_train)

# Making Prediction
pred_svm = model_svm.predict(X_test)

# Calculating Accuracy Score
sv = accuracy_score(y_test, pred_svm)
print(sv)

print(classification_report(y_test,pred_kn))

# confusion Maxtrix
cm6 = confusion_matrix(y_test, pred_svm)
sns.heatmap(cm6/np.sum(cm6), annot = True, fmt=  '0.2%', cmap = 'Greens')

from sklearn.ensemble import AdaBoostClassifier

model_ada = AdaBoostClassifier(learning_rate= 0.002,n_estimators= 205,random_state=42)

model_ada.fit(X_train, y_train)

# Making Prediction
pred_ada = model_ada.predict(X_test)

# Calculating Accuracy Score
ada = accuracy_score(y_test, pred_ada)
print(ada)

print(classification_report(y_test,pred_ada))

# confusion Maxtrix
cm7 = confusion_matrix(y_test, pred_ada)
sns.heatmap(cm7/np.sum(cm7), annot = True, fmt=  '0.2%', cmap = 'Greens')

models = pd.DataFrame({
    'Model':['Logistic Regression', 'Decision Tree', 'Random Forest', 'XGBoost', 'KNeighbours', 'SVM', 'AdaBoost'],
    'Accuracy_score' :[lg, dt, rf, xgb, kn, sv, ada]
})
models
sns.barplot(x='Accuracy_score', y='Model', data=models)

models.sort_values(by='Accuracy_score', ascending=False)